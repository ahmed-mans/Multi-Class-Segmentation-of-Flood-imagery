# Multi-Class Segmentation of Flood Imagery

## üéØ Project Objective
The goal of this project is to build a semantic segmentation pipeline trained on the FloodNet dataset. The task involves classifying each pixel into one of 10 distinct classes, using aerial drone imagery captured in the aftermath of Hurricane Harvey, which caused extensive flooding.

### üìò Workflow
The entire process, from data preprocessing to model training, is implemented in the notebook:
`FloodNet_segmentation_v2.ipynb`.

This notebook includes:

- Data loading and augmentation<br>
- Model architecture and training setup<br>
- Evaluation on the test set.
  
## Dataset Description

The FloodNet dataset consists of a total of __2,343__ images with corresponding segmentation masks.

- __1,843__ images are used for training. However, to train the model, this training set was split into a training and validation set with a 80-20 ratio<br>
- __500__ images are reserved for testing and evaluation.

üè∑Ô∏è Classes
FloodNet features __10__ semantic classes:
```
1. Background  
2. Building Flooded  
3. Building Non-Flooded  
4. Road Flooded  
5. Road Non-Flooded  
6. Water  
7. Tree  
8. Vehicle  
9. Pool  
10. Grass
```
Examples:<br>

![image](https://github.com/user-attachments/assets/6461d545-a1a9-4191-ab2b-f2b9fd1c608f)
 

## üß† Model Architecture

To tackle the semantic segmentation task, the SegFormer model was used, which has a transformer-based architecture for efficient and scalable segmentation.
The implementation is sourced from the official [keras-cv repository](https://github.com/keras-team/keras-cv).

The model is built using the MiT (Mix Transformer) backbone, specifically:

- Backbone: `MiT-B0`<br>
- Pretrained Weights: `mit_b0_imagenet` (pretrained on ImageNet).

## üìà Data Augmentation

With only 1,800+ images available in the training set, the dataset size is insufficient for robust model training and can lead to overfitting. To address this, data augmentation techniques and resizing were applied during the preprocessing stage. These methods help to artificially expand the dataset, improving the model's generalization ability and reducing the risk of overfitting.

```
train_transforms = A.Compose ([
            # Horizontal/vertical flipping
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),  
            A.Rotate(limit=90, p=0.5),

            # Random scaling (zooming in/out)
            A.ShiftScaleRotate(
                shift_limit=0.1,   
                scale_limit=0.2,   
                rotate_limit=0,    
                p=0.5              
            ),

            # Random brightness and contrast adjustment
            A.RandomBrightnessContrast(
                brightness_limit=0.2,  
                contrast_limit=0.2,    
                p=0.5
            ),
        ])
 ```

## ‚öôÔ∏è Training Hyperparameters

- Epochs: __50__<br>
- Optimizer: __Adam__<br>
- Initial Learning Rate: __0.0001__<br>
- LR scheduler: __ReduceLROnPlateau__<br>
- Batch size: __8__<br>

## üìä Model Training Progress

The following graphs show the evolution of accuracy, Dice coefficient, and Dice loss throughout the training process. As observed, all metrics and the loss function reach convergence after approximately __35 epochs__.

![image](https://github.com/user-attachments/assets/94cb5a09-64c0-4cc2-988b-6c259ec66823)



## üì§ Inference on the validation set

To evaluate the model's performance, we compare the **ground truth segmentations** of the validation set images with the **predicted segmentations** generated by the model. This visual comparison helps assess how accurately the model captures the relevant features.

### üîç Observations

The model performs well when segmenting **larger class regions**, such as clusters of **trees**, **grass**, and **water**, accurately identifying and localizing them within the images.  
However, it struggles with **smaller or more detailed objects**, such as **houses**, **vehicles**, and **roads**, where pixel-level classification tends to be less precise.  
Importantly, the model demonstrates strong capability in detecting **flooded areas**, which is a critical feature for emergency response and disaster assessment following events like hurricanes.

![floodnet_inference_1](https://github.com/user-attachments/assets/910d4f6c-b897-470d-830b-b58f3eaa356e)
![floodnet_inference_2](https://github.com/user-attachments/assets/2aea42eb-2880-4449-b818-2055d6e405fc)
![floodnet_inference_3](https://github.com/user-attachments/assets/4f963ca7-6b5f-45a5-9d24-b1e36e387cb6)
![floodnet_inference_4](https://github.com/user-attachments/assets/69ef35f4-848c-4ea5-baa3-fdf01a216139)


## ‚úÖ Accuracy on the test set

The trained model, based on the configuration outlined above, achieved a Dice coefficient of 83.6% on the test set, as evaluated on Kaggle's leaderboard.

![Floodnet_segmentation_leaderbaord](https://github.com/user-attachments/assets/53f18eab-bf4e-4b94-9a1f-42afdf393374)

